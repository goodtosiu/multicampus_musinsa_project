from curl_cffi import requests
import time
import random
import pandas as pd
from tqdm import tqdm  # ì§„í–‰ë¥  ë°” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

def crawl_musinsa_goods(category_code="001", max_pages=1000):
    """
    ë¬´ì‹ ì‚¬ APIë¥¼ í†µí•´ ìƒí’ˆ IDì™€ ì¸ë„¤ì¼ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    :param category_code: ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ì½”ë“œ (ì˜ˆ: 001)
    :param max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    :return: ìˆ˜ì§‘ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    
    collected_data = []
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.musinsa.com/",
        "Accept": "application/json"
    }

    print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: ì¹´í…Œê³ ë¦¬ {category_code}, ìµœëŒ€ {max_pages} í˜ì´ì§€")

    # tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë¥  ë°” ìƒì„±
    pbar = tqdm(range(1, max_pages + 1), unit="page")
    
    for page in pbar:
        # ì§„í–‰ ìƒí™© í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        pbar.set_description(f"ìˆ˜ì§‘ ì¤‘... Page {page}")

        params = {
            "gf": "M",
            "sortCode": "POPULAR",
            "category": category_code,
            "size": 60,
            "testGroup": "",
            "caller": "CATEGORY",
            "page": page,
            "seen": 0,
            "seenAds": ""
        }
        
        url = "https://api.musinsa.com/api2/dp/v1/plp/goods"

        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)
            
            if response.status_code == 200:
                json_data = response.json()
                goods_list = json_data.get('data', {}).get('list', [])
                
                if not goods_list:
                    pbar.write(f"ğŸ›‘ [Page {page}] ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                current_items = 0
                for item in goods_list:
                    item_info = {
                        "goodsNo": item.get("goodsNo"),
                        "thumbnail": item.get("thumbnail"),
                        "goodsName": item.get("goodsName")
                    }
                    collected_data.append(item_info)
                    current_items += 1
                
                # ìš°ì¸¡ì— ìˆ˜ì§‘ëœ ì´ ê°œìˆ˜ í‘œì‹œ ì—…ë°ì´íŠ¸
                pbar.set_postfix(total_collected=len(collected_data), last_count=current_items)

            else:
                pbar.write(f"âš ï¸ [Page {page}] ì—ëŸ¬ ë°œìƒ: Status {response.status_code}")
                break

        except Exception as e:
            pbar.write(f"âŒ [Page {page}] ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

        # ëœë¤ ë”œë ˆì´
        time.sleep(random.uniform(0.2, 0.5))

    return collected_data



# --- ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í˜ì´ì§€ ìˆ˜ ì„¤ì • (ì˜ˆ: 10í˜ì´ì§€)
    target_category = "001"
    target_pages = 1000
    
    result_list = crawl_musinsa_goods(category_code=target_category, max_pages=target_pages)

    print("\n" + "="*40)
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½")
    print(f"- ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: {target_category}")
    print(f"- ì´ ìˆ˜ì§‘ ìƒí’ˆ ìˆ˜: {len(result_list)}ê°œ")
    print("="*40 + "\n")

    # ê²°ê³¼ ìƒ˜í”Œ í™•ì¸
    if result_list:
        print("ğŸ” ìˆ˜ì§‘ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 3ê°œ):")
        for data in result_list[:3]:
            print(data)



            